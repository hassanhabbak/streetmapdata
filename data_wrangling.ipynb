{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Udacity Data Wrangling Final Project\n",
    "\n",
    "In this project, we are to select a city from https://www.openstreetmap.org and use the skills and techniques for data wrangling and cleaning to shape and audit the dataset. The data accuracy and consistensy will be assesed and adjusted as needed. Mongodb will be used to store our dataset and then queried for analytics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The dataset\n",
    "\n",
    "Amsterdam is the city that we will audit and clean. Amsterdam is a major city in the Netherlands and where I live. It is considered to be one of the densest cities in Europe, so it will be interesting to see how clean its data in open street map is.\n",
    "\n",
    "### Data extracted from\n",
    "https://mapzen.com/data/metro-extracts/metro/amsterdam_netherlands/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amsterdam\n",
    "\n",
    "https://en.wikipedia.org/wiki/Amsterdam\n",
    "![title](amsterdam_photo.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of the Data\n",
    "\n",
    "After inserting our cleaned data in MongoDB, we have some questions that we can answer through querying the DB. Those questions are as follows\n",
    ". What is the number of records?\n",
    ". What is the number of nodes in the data?\n",
    ". Who are the top 10 contributers?\n",
    ". What is the top amenity?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the number of records?\n",
    "* Query used:\n",
    "    + db.getCollection('open_street_map').find().count()\n",
    "\n",
    "* Result: 2202741"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the number of Nodes in data?\n",
    "* Query used:\n",
    "    + db.getCollection('open_street_map').find({\"type\":\"node\"}).count()\n",
    "\n",
    "* Result: 2202738"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Who are the top 10 contributers?\n",
    "\n",
    "* Query used:\n",
    "    + db.open_street_map.aggregate([ {$match: {'created.user': {$exists: 1} }},  {$group: {_id: '$created.user', count: {$sum: 1}}},  {$sort: {'count': -1}},  {$limit: 10} ])\n",
    "   \n",
    "   \n",
    "* Result:\n",
    "    + { \"_id\" : \"3dShapes\", \"count\" : 1492463 }\n",
    "    + { \"_id\" : \"padvinder\", \"count\" : 54286 }\n",
    "    + { \"_id\" : \"sebastic\", \"count\" : 46598 }\n",
    "    + { \"_id\" : \"AND\", \"count\" : 44276 }\n",
    "    + { \"_id\" : \"sebastic_BAG\", \"count\" : 40143 }\n",
    "    + { \"_id\" : \"paulbe\", \"count\" : 37989 }\n",
    "    + { \"_id\" : \"AnkEric\", \"count\" : 22327 }\n",
    "    + { \"_id\" : \"Hendrikklaas\", \"count\" : 20008 }\n",
    "    + { \"_id\" : \"Meeuw\", \"count\" : 19828 }\n",
    "    + { \"_id\" : \"rullzer\", \"count\" : 17603 }\n",
    "    \n",
    "\n",
    "* Conclusion:\n",
    "    + Looks like 3dshapes is by far the most contributor. He is more than the top 9 contributors after him put together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the top amenity?\n",
    "* Query used:\n",
    "    + db.open_street_map.aggregate([{$match: {'amenity': {$exists: 1} }},{$group: {_id: '$amenity', count: {$sum: 1}}},{$sort: {'count': -1}},{$limit: 10}])\n",
    "\n",
    "* Result:\n",
    "    + { \"_id\" : \"recycling\", \"count\" : 490 }\n",
    "    + { \"_id\" : \"post_box\", \"count\" : 480 }\n",
    "    + { \"_id\" : \"parking\", \"count\" : 320 }\n",
    "    + { \"_id\" : \"restaurant\", \"count\" : 296 }\n",
    "    + { \"_id\" : \"bench\", \"count\" : 257 }\n",
    "    + { \"_id\" : \"pub\", \"count\" : 171 }\n",
    "    + { \"_id\" : \"fuel\", \"count\" : 147 }\n",
    "    + { \"_id\" : \"fast_food\", \"count\" : 143 }\n",
    "    + { \"_id\" : \"waste_basket\", \"count\" : 103 }\n",
    "    + { \"_id\" : \"cafe\", \"count\" : 97 }\n",
    "    \n",
    "* Conclusion:\n",
    "    + Looks like recycling is the top amenity, followed closely by post_box. Interesting to see how few parkings there is in the data. This could be because of the limited space in amsterdam and the lack of popularity for cars."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Application Code\n",
    "\n",
    "Here we will layout the code that will audit the data and clean it. First we will create a sample dataset from the original dataset to take less time processing it while debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET  # Use cElementTree or lxml if too slow\n",
    "\n",
    "OSM_FILE = \"amsterdam_netherlands.osm\"  # Replace this with your osm file\n",
    "SAMPLE_FILE = \"sample.osm\"\n",
    "\n",
    "k = 10 # Parameter: take every k-th top level element\n",
    "\n",
    "def get_element(osm_file, tags=('node', 'way', 'relation')):\n",
    "    \"\"\"Yield element if it is the right type of tag\n",
    "\n",
    "    Reference:\n",
    "    http://stackoverflow.com/questions/3095434/inserting-newlines-in-xml-file-generated-via-xml-etree-elementtree-in-python\n",
    "    \"\"\"\n",
    "    context = iter(ET.iterparse(osm_file, events=('start', 'end')))\n",
    "    _, root = next(context)\n",
    "    for event, elem in context:\n",
    "        if event == 'end' and elem.tag in tags:\n",
    "            yield elem\n",
    "            root.clear()\n",
    "\n",
    "\n",
    "with open(SAMPLE_FILE, 'wb') as output:\n",
    "    output.write(b'<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n')\n",
    "    output.write(b'<osm>\\n  ')\n",
    "\n",
    "    # Write every kth top level element\n",
    "    for i, element in enumerate(get_element(OSM_FILE)):\n",
    "        if i % k == 0:\n",
    "            output.write(ET.tostring(element, encoding='utf-8'))\n",
    "\n",
    "    output.write(b'</osm>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auditing street names\n",
    "\n",
    "Street names in Dutch language did not make it easy for auditing. Unlike Enlgish, street type is part of the word. After running audit multiple times and iterating on the different type of ways, what remains are mostly street names that do not have a type attached to it. It should not be edited by us since that is a convention in Dutch culture for the street name to already contain its type. If not, it should remain the same.\n",
    "\n",
    "#### Eample valide names\n",
    ". Haarlemrmeerstraat --> The type is straat\n",
    "\n",
    ". Hoofdorplein --> The type is plein\n",
    "\n",
    ". Goude --> No type in the name but still valid\n",
    "\n",
    "Because of this, auditing street types will not be part of the final cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xml.etree.cElementTree as ET\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import pprint\n",
    "\n",
    "OSMFILE = \"sample.osm\"\n",
    "street_type_re = re.compile(r'\\w*(plein|markt|pad|dam|laan|stede|boulevard|zuid|oust|west|noord|hof|hoff|steeg|weide|kade|straat|park|weg|gracht)\\b', re.IGNORECASE)\n",
    "\n",
    "\n",
    "def audit_street_type(street_types, street_name):\n",
    "    \"\"\"Update dictionary of street names and mapping type\"\"\"\n",
    "    m = street_type_re.search(street_name)\n",
    "    if m:\n",
    "        \"\"\"Value already found\"\"\"\n",
    "    else:\n",
    "        if street_name not in street_types:\n",
    "            street_types[street_name].add(street_name)\n",
    "\n",
    "\n",
    "def is_street_name(elem):\n",
    "    return (elem.attrib['k'] == \"addr:street\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auditing Postal Codes\n",
    "\n",
    "The accepted format for Postal codes is 4 digits followed by two letters. Some valid postal codes do not have any letters. The letters should be in captial and preferabbly no spaces. So audit will check for the format, but will automatically correct capitalization and remove spaces.\n",
    "\n",
    "Some of the problems I found were optional postal codes seperated by ';'. For this case I simply selected the first value.\n",
    "\n",
    "Also found cases where the postal code are for wrong cities, this cover most of the erros. All Amsterdam codes starts with 10xx while some of the codes were '2042', '1815' which lie in Zandvoort and Alkamar, cities near Amsterdam. These addresses I do not expect to get in a dataset for Amsterdam, maybe Amsterdam metropolitan area.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "postal_codes = re.compile(r'^1{1}0{1}\\d{2} ?([a-z]{0}|[a-z]{2})$', re.IGNORECASE)\n",
    "bad_postal_codes = []\n",
    "\n",
    "def audit_postal_code(postal_code):\n",
    "    postal_code = postal_code.split(';')[0].upper().replace(\" \", \"\")\n",
    "    if postal_codes.match(postal_code):\n",
    "        return postal_code\n",
    "\n",
    "    bad_postal_codes.append(postal_code)\n",
    "    return None\n",
    "\n",
    "\n",
    "def is_postal_code(address_key):\n",
    "    return address_key == 'addr:postcode'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Format to JSON\n",
    "\n",
    "We then format the data to JSON so that it can be handled easier. We also audit and transform the data while processing it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Bad postal codes:  104613\n"
     ]
    }
   ],
   "source": [
    "import codecs\n",
    "import json\n",
    "\n",
    "lower = re.compile(r'^([a-z]|_)*$')\n",
    "lower_colon = re.compile(r'^([a-z]|_)*:([a-z]|_)*$')\n",
    "problemchars = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]')\n",
    "\n",
    "CREATED = [\"version\", \"changeset\", \"timestamp\", \"user\", \"uid\"]\n",
    "ATTRIB = [\"id\", \"visible\", \"amenity\", \"cuisine\", \"name\", \"phone\"]\n",
    "\n",
    "\n",
    "def shape_element(element):\n",
    "    \"\"\"\n",
    "    Parse, validate and format node and way xml elements.\n",
    "    Return list of dictionaries\n",
    "    Keyword arguments:\n",
    "    element -- element object from xml element tree iterparse\n",
    "    \"\"\"\n",
    "    if element.tag == 'node' or element.tag == 'way':\n",
    "\n",
    "        # Add empty created dictionary and k/v = type: node/way\n",
    "        node = {'created': {}, 'type': element.tag}\n",
    "\n",
    "        # Update pos array with lat and lon\n",
    "        if 'lat' in element.attrib and 'lon' in element.attrib:\n",
    "            node['pos'] = [float(element.attrib['lat']), float(element.attrib['lon'])]\n",
    "\n",
    "        # Deal with node and way attributes\n",
    "        for k in element.attrib:\n",
    "\n",
    "            if k == 'lat' or k == 'lon':\n",
    "                continue\n",
    "            if k in CREATED:\n",
    "                node['created'][k] = element.attrib[k]\n",
    "            else:\n",
    "                # Add direct key/value items of node/way\n",
    "                node[k] = element.attrib[k]\n",
    "\n",
    "        # Deal with second level tag items\n",
    "        for tag in element.iter('tag'):\n",
    "            k = tag.attrib['k']\n",
    "            v = tag.attrib['v']\n",
    "\n",
    "            # Search for problem characters in 'k' and ignore them\n",
    "            if problemchars.search(k):\n",
    "                # Add to array to print out later\n",
    "                continue\n",
    "            elif k.startswith('addr:'):\n",
    "                address = k.split(':')\n",
    "                if len(address) == 2:\n",
    "                    if 'address' not in node:\n",
    "                        node['address'] = {}\n",
    "                    if is_postal_code(k):\n",
    "                        v = audit_postal_code(v)\n",
    "                        if v == None:\n",
    "                            return None\n",
    "                    node['address'][address[1]] = v\n",
    "            else:\n",
    "                node[k] = v\n",
    "\n",
    "        # Add key/value node ref from way\n",
    "        node_refs = []\n",
    "        for nd in element.iter('nd'):\n",
    "            node_refs.append(nd.attrib['ref'])\n",
    "\n",
    "        if len(node_refs) > 0:\n",
    "            node['node_refs'] = node_refs\n",
    "\n",
    "        return node\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def process_map(file_in, pretty=False):\n",
    "    # You do not need to change this file\n",
    "    file_out = \"{0}.json\".format(file_in)\n",
    "    with codecs.open(file_out, \"w\") as fo:\n",
    "        for _, element in ET.iterparse(file_in):\n",
    "            el = shape_element(element)\n",
    "            if el:\n",
    "                if pretty:\n",
    "                    fo.write(json.dumps(el, indent=2) + \"\\n\")\n",
    "                else:\n",
    "                    fo.write(json.dumps(el) + \"\\n\")\n",
    "\n",
    "\n",
    "process_map('sample.osm', True)\n",
    "\n",
    "print(\"Total Bad postal codes: \", len(bad_postal_codes))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "We had to ignore a large section of the downloaded Amsterdam dataset. The dataset came with a bounding box instead of an actual boundary. But once the data was cleaned to only consider amsterdam area data, it seems that the data is quite accurate and complete.\n",
    "\n",
    "I would recommend a standardization of the postal code data from openstreetmap to just make validation easier. That includes correcting capitalization and removing spaces.\n",
    "\n",
    "While I feel the data cleaning aspect of this was not significant enough to submit to openstreetmap due to the cleaness of the data already provided. I assume it is because of a large and active community in Amsterdam that contributed ot the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:data_analysis]",
   "language": "python",
   "name": "conda-env-data_analysis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
